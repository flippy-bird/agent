## 开源项目学习



### 1. agentUniverse

> https://github.com/agentuniverse-ai/agentUniverse



### 2. youtu-agent

> https://github.com/TencentCloudADP/youtu-agent

项目说是基于OpenAI agent进行构建的，看了一下，主要的特色是 yaml，通过加载不同的配置来加载不同功能的agent；agent源码部分直接看OpenAI agent好了，后续再看这个项目



### 3. Cline

> https://github.com/cline/cline

源码理解：[cline源代码分析](https://zhuanlan.zhihu.com/p/20474368968)



### 4. smolagents

> https://github.com/huggingface/smolagents



#### memory 管理

```python
class AgentMemory:
    def __init__(self, system_prompt:str):
        self.system_prompt: SystemPromptStep = SystemPromptStep(system_prompt=system_prompt)
        self.steps: list[TaskStep | ActionStep | PlanningStep] = []
```

将LLM执行过程的信息划分成了四个部分(主要，其它)：

![image-20250926172929838](https://raw.githubusercontent.com/nashpan/image-hosting/main/image-20250926172929838.png)

- TaskStep: 与用户输入相关 (用户提问，上传图片等)
- SystemPromptStep: 系统的prompt
- PlanningStep: 与规划相关的记忆 (**暂时还没遇到**)
- ActionStep：当前送到LLM进行执行的信息

**历史信息的获取**

所以主要在于上面每种类型记忆数据 `to_messages`的实现 需要注意的是前后信息的完整性

```python
# agents.py line:1256
def _step_stream(
        self, memory_step: ActionStep
    ) -> Generator[ChatMessageStreamDelta | ToolCall | ToolOutput | ActionOutput]:
    """
    Perform one step in the ReAct framework: the agent thinks, acts, and observes the result.
    Yields ChatMessageStreamDelta during the run if streaming is enabled.
    At the end, yields either None if the step is not final, or the final answer.
    """
    memory_messages = self.write_memory_to_messages()
        
# agents.py line:758
def write_memory_to_messages(
        self,
        summary_mode: bool = False,
    ) -> list[ChatMessage]:
    """
    Reads past llm_outputs, actions, and observations or errors from the memory into a series of messages 	  that can be used as input to the LLM. Adds a number of keywords (such as PLAN, error, etc) to help 	 the LLM.
    """
    messages = self.memory.system_prompt.to_messages(summary_mode=summary_mode)
    for memory_step in self.memory.steps:
        messages.extend(memory_step.to_messages(summary_mode=summary_mode))
    return messages
```



#### 边界处理

当超过最大尝试次数（默认是20次）时，最后会总结19步的step，然后给出一个最终答案

```python
# agent.py  line:810

def provide_final_answer(self, task: str) -> ChatMessage:
    
    messages : 这里有一个专门针对这种情况的系统prompt
    messages += self.write_memory_to_messages()[1:]
    messages : 需要组装的post_messages
    
    try:
        chat_message: ChatMessage = self.model.generate(messages)
        return chat_message
    except Exception as e:
        return ChatMessage(
            role=MessageRole.ASSISTANT,
            content=[{"type": "text", "text": f"Error in generating final LLM output: {e}"}],
        )
```



#### Agent: CodeAgent

![image-20250929112702011](https://raw.githubusercontent.com/nashpan/image-hosting/main/image-20250929112702011.png)

项目的CodeAgent模式算是这个项目里面比较新颖的一种方式了，调用工具的API是通过执行python代码的方式来执行的，但是感觉解析python的AST那部分，就感觉好复杂-_-#，没有FunctionCall的这种方式简洁了。

来源于[Executable Code Actions Elicit Better LLM Agents](https://link.zhihu.com/?target=https%3A//huggingface.co/papers/2402.01030)，这篇论文主要的出发动机是当前LLM Agent通常通过以预定义的格式生成 JSON 或文本来生成Action，这通常受到**约束动作空间（例如，预定义工具的范围）和受限灵活性（例如，无法组合多个工具）的限制**。

> 链接：https://zhuanlan.zhihu.com/p/16341067315



### 5. openhands

> https://github.com/All-Hands-AI/OpenHands/tree/main/openhands



### 6. DeepResearchAgent

> https://github.com/SkyworkAI/DeepResearchAgent



似乎是按照**Plan and Execute 的设计模式**来规划整体结构的

![image-20251014140304860](https://raw.githubusercontent.com/nashpan/image-hosting/main/image-20251014140304860.png)

#### mcp 工具

呃呃呃，居然将一些功能代码写在了json文件中，亏我找了半天这些mcp的函数在哪里，居然就在json文件中，在`script_content`这个字段

#### 总体架构

基础的Agent实现完全是基于Smolagents来进行实现的，包括agent的逻辑，memory等

**上面的架构体现也完全是由prompt来体现的**, 这么一看，所谓的agent架构上的创新，完全是prompt工程，底层还是特别依赖LLM对于prompt的执行能力；

使用了mmengine的注册机制来注册写在yaml配置文件里的不同agent和不同tools，算是一个小的点

```python
# 繼承關係圖： 不同的agent --->  genera_agent ---> async_multistep_agent
```







### 7. OpenAI-Agent

> https://github.com/openai/openai-agents-python



### 8. AgentScope (阿里的) 

> https://github.com/agentscope-ai/agentscope

#### 记忆

长期记忆部分使用了mem0这个工具，当然，代码里面也提到了，可以使用阿里自家的ReMe这个记忆框架

#### Agent

这一块儿使用的是基本的React模式，输出最后的回答，也成了一个工具；多了一个

```python
### AgentBase
# 在AgentBase里面有一个虚函数 reply
def reply()

### 在ReactAgentBase里面有基本的React框架
def _reasoning()   # 虚函数

def _acting()      # 虚函数

### 在ReactAgent里面
def reply():           这里就类似OpenManus里面的step()函数的作用了，从源码来看，逻辑完全一样
    self._reasoning()
    ...
    self._acting()
```

在Agent里面有一个observe，感觉是为了观察到外界信息准备的接口(用于多Agent之间的信息互动)

```python
async def observe(self, msg: Msg | list[Msg] | None) -> None:
    """Receive the given message(s) without generating a reply.

    Args:
        msg (`Msg | list[Msg] | None`):
            The message(s) to be observed.
    """
    raise NotImplementedError(
        f"The observe function is not implemented in"
        f" {self.__class__.__name__} class.",
    )
```

#### 多Agent互动

在这个框架里面使用的是swarm模式，似乎比较简单，每个agent observe其它agent的输出，添加到自己的记忆里面去就好了

在src/pipeline/_msghub.py文件夹里面 （或者见类名时的说明）

```python
async def broadcast(self, msg: list[Msg] | Msg) -> None:
    """Broadcast the message to all participants.

    Args:
        msg (`list[Msg] | Msg`):
            Message(s) to be broadcast among all participants.
    """
    for agent in self.participants:
        await agent.observe(msg)
```

#### Interrupt(中断介入)

这个好像还不错，可以看一下

#### Plan的实现